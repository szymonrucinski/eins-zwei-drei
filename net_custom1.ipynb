{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from utils import plot_confusion_matrix\n",
    "from torchviz import make_dot\n",
    "from torchsummary import summary\n",
    "import torchvision.transforms.functional as TF\n",
    "from utils import pytorchtools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "convert_input = lambda images : images.view(images.shape[0], -1).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 784\n",
    "num_classes = 10\n",
    "num_epochs = 100\n",
    "patience = 10\n",
    "batch_size = 512\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MNIST dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                transforms.Normalize((0.5,), (0.5,)),\n",
    "                              ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = torchvision.datasets.MNIST(root='..//set', \n",
    "                                           train=True, \n",
    "                                           transform=transform,  \n",
    "                                           download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = torchvision.datasets.MNIST(root='..///set', \n",
    "                                          train=False, \n",
    "                                          transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data loader\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
    "                                           batch_size=batch_size, \n",
    "                                           shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n",
    "                                          batch_size=batch_size, \n",
    "                                          shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fully connected neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNet, self).__init__()\n",
    "        \n",
    "        #layers\n",
    "        self.l1 = nn.Linear(784, 256) # layer 1\n",
    "        self.l2 = nn.Linear(256, 128) # layer 2\n",
    "        self.l3 = nn.Linear(128, 64) # layer 3\n",
    "        self.l4 = nn.Linear(64, 10) # layer 4\n",
    "        \n",
    "        self.sigmoid = nn.Sigmoid() \n",
    "        self.relu = nn.ReLU()\n",
    "        self.softmax = nn.LogSoftmax(dim = 1) \n",
    "     \n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.l1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.l2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.l3(x)\n",
    "        x = self.sigmoid(x)\n",
    "        x = self.l4(x)\n",
    "        x = self.softmax(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NeuralNet().to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Linear-1               [-1, 1, 256]         200,960\n",
      "              ReLU-2               [-1, 1, 256]               0\n",
      "            Linear-3               [-1, 1, 128]          32,896\n",
      "              ReLU-4               [-1, 1, 128]               0\n",
      "            Linear-5                [-1, 1, 64]           8,256\n",
      "           Sigmoid-6                [-1, 1, 64]               0\n",
      "            Linear-7                [-1, 1, 10]             650\n",
      "        LogSoftmax-8                [-1, 1, 10]               0\n",
      "================================================================\n",
      "Total params: 242,762\n",
      "Trainable params: 242,762\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.01\n",
      "Params size (MB): 0.93\n",
      "Estimated Total Size (MB): 0.94\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "summary(model, (1, 784))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to track the training loss as the model trains\n",
    "train_losses = []\n",
    "# to track the validation loss as the model trains\n",
    "valid_losses = []\n",
    "# to track the average training loss per epoch as the model trains\n",
    "avg_train_losses = []\n",
    "# to track the average validation loss per epoch as the model trains\n",
    "avg_valid_losses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000\n",
      "train_loss: 1.12714 valid_loss: 0.52604\n",
      "Validation loss decreased (inf --> 0.526040).  Saving model ...\n",
      "0\n",
      "train_loss: 0.39695 valid_loss: 0.31223\n",
      "Validation loss decreased (0.526040 --> 0.312230).  Saving model ...\n",
      "1\n",
      "train_loss: 0.26246 valid_loss: 0.23153\n",
      "Validation loss decreased (0.312230 --> 0.231526).  Saving model ...\n",
      "2\n",
      "train_loss: 0.19663 valid_loss: 0.17682\n",
      "Validation loss decreased (0.231526 --> 0.176822).  Saving model ...\n",
      "3\n",
      "train_loss: 0.15630 valid_loss: 0.16701\n",
      "Validation loss decreased (0.176822 --> 0.167012).  Saving model ...\n",
      "4\n",
      "train_loss: 0.12475 valid_loss: 0.12786\n",
      "Validation loss decreased (0.167012 --> 0.127862).  Saving model ...\n",
      "5\n",
      "train_loss: 0.10547 valid_loss: 0.11854\n",
      "Validation loss decreased (0.127862 --> 0.118535).  Saving model ...\n",
      "6\n",
      "train_loss: 0.09154 valid_loss: 0.11108\n",
      "Validation loss decreased (0.118535 --> 0.111083).  Saving model ...\n",
      "7\n",
      "train_loss: 0.07765 valid_loss: 0.09543\n",
      "Validation loss decreased (0.111083 --> 0.095431).  Saving model ...\n",
      "8\n",
      "train_loss: 0.06851 valid_loss: 0.09785\n",
      "EarlyStopping counter: 1 out of 10\n",
      "9\n",
      "train_loss: 0.05969 valid_loss: 0.08838\n",
      "Validation loss decreased (0.095431 --> 0.088379).  Saving model ...\n",
      "10\n",
      "train_loss: 0.05332 valid_loss: 0.08957\n",
      "EarlyStopping counter: 1 out of 10\n",
      "11\n",
      "train_loss: 0.04929 valid_loss: 0.08792\n",
      "Validation loss decreased (0.088379 --> 0.087921).  Saving model ...\n",
      "12\n",
      "train_loss: 0.04632 valid_loss: 0.07691\n",
      "Validation loss decreased (0.087921 --> 0.076905).  Saving model ...\n",
      "13\n",
      "train_loss: 0.04074 valid_loss: 0.08172\n",
      "EarlyStopping counter: 1 out of 10\n",
      "14\n",
      "train_loss: 0.03656 valid_loss: 0.07684\n",
      "Validation loss decreased (0.076905 --> 0.076841).  Saving model ...\n",
      "15\n",
      "train_loss: 0.03070 valid_loss: 0.07309\n",
      "Validation loss decreased (0.076841 --> 0.073090).  Saving model ...\n",
      "16\n",
      "train_loss: 0.02701 valid_loss: 0.07530\n",
      "EarlyStopping counter: 1 out of 10\n",
      "17\n",
      "train_loss: 0.02392 valid_loss: 0.07134\n",
      "Validation loss decreased (0.073090 --> 0.071342).  Saving model ...\n",
      "18\n",
      "train_loss: 0.02391 valid_loss: 0.07850\n",
      "EarlyStopping counter: 1 out of 10\n",
      "19\n",
      "train_loss: 0.02134 valid_loss: 0.07467\n",
      "EarlyStopping counter: 2 out of 10\n",
      "20\n",
      "train_loss: 0.02212 valid_loss: 0.07368\n",
      "EarlyStopping counter: 3 out of 10\n",
      "21\n",
      "train_loss: 0.01855 valid_loss: 0.07419\n",
      "EarlyStopping counter: 4 out of 10\n",
      "22\n",
      "train_loss: 0.01569 valid_loss: 0.08180\n",
      "EarlyStopping counter: 5 out of 10\n",
      "23\n",
      "train_loss: 0.01651 valid_loss: 0.07090\n",
      "Validation loss decreased (0.071342 --> 0.070897).  Saving model ...\n",
      "24\n",
      "train_loss: 0.01382 valid_loss: 0.07504\n",
      "EarlyStopping counter: 1 out of 10\n",
      "25\n",
      "train_loss: 0.01001 valid_loss: 0.07180\n",
      "EarlyStopping counter: 2 out of 10\n",
      "26\n",
      "train_loss: 0.01124 valid_loss: 0.07912\n",
      "EarlyStopping counter: 3 out of 10\n",
      "27\n",
      "train_loss: 0.01578 valid_loss: 0.09389\n",
      "EarlyStopping counter: 4 out of 10\n",
      "28\n",
      "train_loss: 0.01231 valid_loss: 0.07781\n",
      "EarlyStopping counter: 5 out of 10\n",
      "29\n",
      "train_loss: 0.01740 valid_loss: 0.07840\n",
      "EarlyStopping counter: 6 out of 10\n",
      "30\n",
      "train_loss: 0.00845 valid_loss: 0.07523\n",
      "EarlyStopping counter: 7 out of 10\n",
      "31\n",
      "train_loss: 0.00590 valid_loss: 0.07653\n",
      "EarlyStopping counter: 8 out of 10\n",
      "32\n",
      "train_loss: 0.00535 valid_loss: 0.08013\n",
      "EarlyStopping counter: 9 out of 10\n",
      "33\n",
      "train_loss: 0.00656 valid_loss: 0.08062\n",
      "EarlyStopping counter: 10 out of 10\n",
      "34\n",
      "Early stopping\n"
     ]
    }
   ],
   "source": [
    "size = len(train_loader.dataset)\n",
    "print(size)\n",
    "\n",
    "# initialize the early_stopping object\n",
    "early_stopping = pytorchtools.EarlyStopping(patience=patience, verbose=True)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    ##########################    \n",
    "    #######TRAIN MODEL########\n",
    "    ##########################\n",
    "    epochs_loss=0\n",
    "    for i, (images, labels) in enumerate(train_loader):  \n",
    "        # Move tensors to the configured device\n",
    "        images = images.reshape(-1, 28*28).to(device)\n",
    "#         images = images.view(images.shape[0], -1).to(device)\n",
    "        labels = labels.to(device)\n",
    "#         print(images.shape)\n",
    "        # Forward pass\n",
    "        outputs = model(images).to(device)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backprpagation and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        \n",
    "        #calculate train_loss\n",
    "        train_losses.append(loss.item())\n",
    "        optimizer.step()\n",
    "    \n",
    "    ##########################    \n",
    "    #####VALIDATE MODEL#######\n",
    "    ##########################\n",
    "    model.eval()\n",
    "    for images, labels in test_loader:\n",
    "        images = images.reshape(-1, 28*28).to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(images).to(device)\n",
    "        loss = criterion(outputs,labels)\n",
    "        valid_losses.append(loss.item())\n",
    "    \n",
    "    # print training/validation statistics \n",
    "    # calculate average loss over an epoch\n",
    "    train_loss = numpy.average(train_losses)\n",
    "    valid_loss = numpy.average(valid_losses)\n",
    "#     print(train_loss)\n",
    "    avg_train_losses.append(train_loss)\n",
    "    avg_valid_losses.append(valid_loss)\n",
    "    \n",
    "    print_msg = (f'train_loss: {train_loss:.5f} ' + f'valid_loss: {valid_loss:.5f}')\n",
    "    \n",
    "    print(print_msg)\n",
    "\n",
    "    \n",
    "    # clear lists to track next epoch\n",
    "    train_losses = []\n",
    "    valid_losses = []\n",
    "    \n",
    "    early_stopping(valid_loss, model)\n",
    "    print(epoch)\n",
    "        \n",
    "    if early_stopping.early_stop:\n",
    "        print(\"Early stopping\")\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load the last checkpoint with the best model\n",
    "model.load_state_dict(torch.load('checkpoint.pt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss function vs Epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure()\n",
    "\n",
    "######1st plot#########\n",
    "ax1 = fig.add_subplot()\n",
    "ax1.set_ylabel('valid  /  train loss')\n",
    "ax1.set_xlabel('number of epochs')\n",
    "halt = avg_valid_losses.index(min(avg_valid_losses))\n",
    "\n",
    "\n",
    "plt.axvline(x=halt, color='r', linestyle=\"--\", label=\"stop training\")\n",
    "\n",
    "print(avg_valid_losses.index(min(avg_valid_losses)))\n",
    "\n",
    "\n",
    "plt.plot(list(range(len(avg_valid_losses))), avg_valid_losses, label = \"valid loss\")\n",
    "plt.plot(list(range(len(avg_valid_losses))), avg_train_losses, label=\"train loss\")\n",
    "\n",
    "h,labels = ax1.get_legend_handles_labels()\n",
    "labels[:1] = ['stop training','valid loss', 'train loss',]\n",
    "ax1.legend(labels=labels)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 10000 test images: 98.05 %\n"
     ]
    }
   ],
   "source": [
    "# In the test phase, don't need to compute gradients (for memory efficiency)\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in test_loader:\n",
    "        images = images.reshape(-1, 28*28).to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    print('Accuracy of the network on the 10000 test images: {} %'.format(100 * correct / total))\n",
    "\n",
    "# Save the model checkpoint\n",
    "torch.save(model.state_dict(), '.\\\\models\\\\net_custom1`.ckpt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def get_all_preds(model, loader):\n",
    "    all_preds = torch.tensor([]).to(device)\n",
    "    for batch in loader:\n",
    "        images, labels = batch\n",
    "        images = images.reshape(-1,28*28).to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        preds = model(images).to(device)\n",
    "        all_preds = torch.cat(\n",
    "            (all_preds, preds)\n",
    "            ,dim=0\n",
    "        )\n",
    "    return all_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total correct 9805\n",
      "accuracy 0.9805\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    prediction_loader = torch.utils.data.DataLoader(test_dataset, batch_size =10000)\n",
    "    train_preds = get_all_preds(model, prediction_loader).to(device)\n",
    "    \n",
    "train_preds.shape\n",
    "train_preds.grad_fn\n",
    "train_preds.grad\n",
    "\n",
    "def get_num_correct(preds, labels):\n",
    "    return preds.argmax(dim=1).eq(labels).sum().item()\n",
    "\n",
    "preds_correct = get_num_correct(train_preds, test_dataset.targets.to(device) )\n",
    "print('total correct', preds_correct)\n",
    "print('accuracy', preds_correct / len(test_dataset))\n",
    "train_preds.is_cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "stacked = torch.stack((test_dataset.targets.to(device), train_preds.argmax(dim=1)), dim=1)\n",
    "stacked[9].tolist()\n",
    "cmt = torch.zeros(10,10, dtype=torch.int64)\n",
    "for p in stacked:\n",
    "    j,k = p.tolist()\n",
    "    cmt[j,k] = cmt[j, k] + 1\n",
    "    \n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "lb = test_dataset.targets.detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7 2 1 ... 4 5 6]\n"
     ]
    }
   ],
   "source": [
    "rp = train_preds.argmax(dim=1).detach().cpu().numpy()\n",
    "print(rp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(lb, rp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'module' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-50-b08a8278cb48>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mnames\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mplot_confusion_matrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcm\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnames\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: 'module' object is not callable"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 720x720 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "names = set(lb)\n",
    "plt.figure(figsize=(10,10))\n",
    "plot_confusion_matrix(cm, names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataiter = iter(train_loader) # creating a iterator\n",
    "images, labels = dataiter.next()\n",
    "figure = plt.figure()\n",
    "num_of_images = 60\n",
    "for index in range(1, num_of_images + 1):\n",
    "    model.predict(images[index]).to(device)\n",
    "    plt.subplot(6, 10, index)\n",
    "    plt.imshow(images[index].numpy().squeeze(), cmap='gray_r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, device, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            target = target.to(device)\n",
    "            data = data.reshape(-1,28*28).to(device)\n",
    "            output = model(data)\n",
    "            \n",
    "            test_loss += nn.functional.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n",
    "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "            # Store wrongly predicted images\n",
    "            wrong_idx = (pred != target.view_as(pred)).nonzero()[:, 0]\n",
    "            wrong_samples = data[wrong_idx]\n",
    "            wrong_preds = pred[wrong_idx]\n",
    "            actual_preds = target.view_as(pred)[wrong_idx]\n",
    "\n",
    "            for i in range(len(wrong_idx)):\n",
    "                sample = wrong_samples[i]\n",
    "                wrong_pred = wrong_preds[i]\n",
    "                actual_pred = actual_preds[i]\n",
    "                # Undo normalization\n",
    "#                 print(wrong_samples[i].shape)\n",
    "                sample = sample.reshape(28,28).to(device)\n",
    "                sample = sample * 0.3081\n",
    "                sample = sample + 0.1307\n",
    "                sample = sample * 255.\n",
    "                sample = sample.byte()\n",
    "                img = TF.to_pil_image(sample)\n",
    "                path = '.\\\\mistakes\\\\net_custom1\\\\'\n",
    "                img.save(path+'wrong_idx{}_pred{}_actual{}.png'.format(\n",
    "                    wrong_idx[i], wrong_pred.item(), actual_pred.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test(model,device, test_loader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
